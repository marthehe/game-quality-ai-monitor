import os
import json
from datetime import datetime, timedelta
import openai
from dotenv import load_dotenv

# üì¶ Load environment variables from the .env file (for sensitive info like API keys)
load_dotenv()

# üîê Get the OpenAI API key from the environment
openai.api_key = os.getenv("OPENAI_API_KEY")

# üìÑ Path to the telemetry log file (generated by the telemetry simulator)
LOG_FILE = "../telemetry_simulator/telemetry_log.json"

# üïê Time window to filter recent crash logs (in minutes)
TIME_WINDOW_MINUTES = 5

# üß† Step 1: Read crash logs that occurred within the last N minutes
def read_recent_crashes():
    now = datetime.utcnow()  # Get the current UTC time
    window_start = now - timedelta(minutes=TIME_WINDOW_MINUTES)  # Calculate the time window

    try:
        with open(LOG_FILE, "r") as f:
            logs = [json.loads(line) for line in f.readlines()]  # Read and parse all lines as JSON
    except FileNotFoundError:
        print("[ERROR] Log file not found.")  # Handle missing file gracefully
        return []

    # Filter only crash events that are within the last N minutes
    return [
        log for log in logs
        if log["event_type"] == "crash"
        and datetime.fromisoformat(log["timestamp"]) >= window_start
    ]

# ‚úèÔ∏è Step 2: Build the prompt to send to GPT based on crash log data
def build_prompt(crashes):
    prompt = (
        "You are a QA assistant at Xbox. Based on the following crash logs, "
        "summarize the issues, possible root causes, and suggest test steps to reproduce the issues.\n\n"
        f"Crash Logs:\n{json.dumps(crashes, indent=2)}\n\n"
        "Summary:"
    )
    return prompt

# ü§ñ Step 3: Send prompt to GPT model and receive QA summary
def get_ai_summary(crashes):
    if not crashes:
        return "[INFO] No recent crash data to summarize."  # Avoid sending empty logs

    prompt = build_prompt(crashes)  # Prepare the full prompt with logs

    try:
        # Call OpenAI's chat completion endpoint with the prepared prompt
        response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",  # You can change to gpt-4 if available
            messages=[
                {"role": "system", "content": "You are a helpful QA assistant."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,       # Low temperature = more focused and consistent answers
            max_tokens=500         # Max size of the response
        )
        return response['choices'][0]['message']['content']  # Return GPT‚Äôs reply
    except Exception as e:
        return f"[ERROR] Failed to get AI response: {e}"

# üöÄ Step 4: Main execution logic
def main():
    crashes = read_recent_crashes()  # Read filtered crash logs
    print(f"[DEBUG] Found {len(crashes)} crash logs in the last {TIME_WINDOW_MINUTES} minutes.")

    summary = get_ai_summary(crashes)  # Send logs to GPT and get a summary

    print("\n--- AI QA Summary ---\n")
    print(summary)  # Print summary to terminal

    # üíæ Save the summary to a Markdown file if it contains valid info
    if "No recent crash data" not in summary:
        timestamp = datetime.utcnow().strftime("%Y-%m-%d %H:%M:%S UTC")
        output_path = "crash_summary.md"

        # Append the summary with a timestamp to crash_summary.md
        with open(output_path, "a", encoding="utf-8") as f:
            f.write(f"## Crash Summary - {timestamp}\n\n")
            f.write(summary.strip() + "\n\n---\n\n")

        print(f"[INFO] Summary saved to {output_path}")
    else:
        print("[INFO] Nothing to save ‚Äî no crash data.")

# ‚ñ∂Ô∏è Run the main function if this script is executed directly
if __name__ == "__main__":
    main()

